<img src="./saoa.png"></img>

## Self Attention on Attention - Pytorch

A Pytorch implementation of Self Attention on Attention module, from the paper <a href="https://arxiv.org/abs/2011.02164v1">An Improved Attention for Visual Question Answering</a>. This module was used to achieve new SOTA on visual question / answering. It will also include the Guided Attention on Attention (GAoA) module, the cross-attention variant.

## Citations

```bibtex
@misc{rahman2020improved,
    title   = {An Improved Attention for Visual Question Answering}, 
    author  = {Tanzila Rahman and Shih-Han Chou and Leonid Sigal and Giuseppe Carenini},
    year    = {2020},
    eprint  = {2011.02164},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
```
